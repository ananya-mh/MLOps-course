{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d13518e9-7903-4393-ab58-6027b10c5936",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the TensorBoard notebook extension.\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "565b05a0-4a69-44f7-9370-e2f19f94f8cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c238774-e8f7-46b2-b8d9-3ff7d5da7a31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Enabled dumping callback in thread MainThread (dump root: ./logs/, tensor debug mode: FULL_HEALTH)\n",
      "TensorFlow version:  2.20.0\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from packaging import version\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "tf.debugging.experimental.enable_dump_debug_info('./logs/',\n",
    "                                                 tensor_debug_mode=\"FULL_HEALTH\", \n",
    "                                                 circular_buffer_size=-1)\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "\n",
    "print(\"TensorFlow version: \", tf.__version__)\n",
    "assert version.parse(tf.__version__).release[0] >= 2, \\\n",
    "    \"This notebook requires TensorFlow 2.0 or above.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6285810b-541b-4fba-a4cf-674824c9bbda",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_size = 1000\n",
    "# 80% of the data is for training.\n",
    "train_pct = 0.8\n",
    "\n",
    "train_size = int(data_size * train_pct)\n",
    "\n",
    "# Create some input data between -1 and 1 and randomize it.\n",
    "x = np.linspace(-1, 1, data_size)\n",
    "np.random.shuffle(x)\n",
    "\n",
    "# Generate the output data.\n",
    "# y = 0.5x + 2 + noise\n",
    "y = 0.5 * x + 2 + np.random.normal(0, 0.05, (data_size, ))\n",
    "\n",
    "# Split into test and train pairs.\n",
    "x_train, y_train = x[:train_size], y[:train_size]\n",
    "x_test, y_test = x[train_size:], y[train_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0758b203-9ef0-4f93-92ca-d8c938f2dc35",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Failed to read source code from path: C:\\Users\\ANANYA\\AppData\\Local\\Temp\\ipykernel_22832\\384751159.py. Reason: Source path neither exists nor can be loaded as a .par file: C:\\Users\\ANANYA\\AppData\\Local\\Temp\\ipykernel_22832\\384751159.py\n",
      "Training simple model...\n",
      "Epoch 1/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5s/step - loss: 4.2006 - mae: 2.0041 - val_loss: 0.1010 - val_mae: 0.2693\n",
      "Epoch 2/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 961ms/step - loss: 0.0969 - mae: 0.2596 - val_loss: 0.0248 - val_mae: 0.1358\n",
      "Epoch 3/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 944ms/step - loss: 0.0274 - mae: 0.1425 - val_loss: 0.0077 - val_mae: 0.0734\n",
      "Epoch 4/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 943ms/step - loss: 0.0070 - mae: 0.0689 - val_loss: 0.0030 - val_mae: 0.0434\n",
      "Epoch 5/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 943ms/step - loss: 0.0036 - mae: 0.0477 - val_loss: 0.0028 - val_mae: 0.0416\n",
      "Epoch 6/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 964ms/step - loss: 0.0028 - mae: 0.0421 - val_loss: 0.0024 - val_mae: 0.0389\n",
      "Epoch 7/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 957ms/step - loss: 0.0026 - mae: 0.0408 - val_loss: 0.0025 - val_mae: 0.0393\n",
      "Epoch 8/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 963ms/step - loss: 0.0026 - mae: 0.0407 - val_loss: 0.0024 - val_mae: 0.0391\n",
      "Epoch 9/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 0.0026 - mae: 0.0406 - val_loss: 0.0024 - val_mae: 0.0392\n",
      "Epoch 10/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 0.0026 - mae: 0.0406 - val_loss: 0.0024 - val_mae: 0.0392\n",
      "Epoch 11/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 998ms/step - loss: 0.0026 - mae: 0.0406 - val_loss: 0.0024 - val_mae: 0.0392\n",
      "Epoch 12/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 993ms/step - loss: 0.0026 - mae: 0.0406 - val_loss: 0.0024 - val_mae: 0.0392\n",
      "Epoch 13/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 0.0026 - mae: 0.0406 - val_loss: 0.0024 - val_mae: 0.0392\n",
      "Epoch 14/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 0.0026 - mae: 0.0406 - val_loss: 0.0024 - val_mae: 0.0392\n",
      "Epoch 15/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 0.0026 - mae: 0.0406 - val_loss: 0.0024 - val_mae: 0.0392\n",
      "Epoch 16/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 982ms/step - loss: 0.0026 - mae: 0.0406 - val_loss: 0.0024 - val_mae: 0.0392\n",
      "Epoch 17/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 0.0026 - mae: 0.0406 - val_loss: 0.0024 - val_mae: 0.0392\n",
      "Epoch 18/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 980ms/step - loss: 0.0026 - mae: 0.0406 - val_loss: 0.0024 - val_mae: 0.0392\n",
      "Epoch 19/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step - loss: 0.0026 - mae: 0.0406 - val_loss: 0.0024 - val_mae: 0.0392\n",
      "Epoch 20/20\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step - loss: 0.0026 - mae: 0.0406 - val_loss: 0.0024 - val_mae: 0.0392\n",
      "Simple model - Average test loss: 0.0088\n",
      "\n",
      "Training deep model...\n",
      "Epoch 1/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 123ms/step - loss: 0.8603 - mae: 0.7079 - val_loss: 0.0450 - val_mae: 0.1614\n",
      "Epoch 2/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - loss: 0.1081 - mae: 0.2546 - val_loss: 0.0195 - val_mae: 0.1296\n",
      "Epoch 3/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - loss: 0.0556 - mae: 0.1888 - val_loss: 0.0193 - val_mae: 0.1284\n",
      "Epoch 4/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 0.0430 - mae: 0.1632 - val_loss: 0.0065 - val_mae: 0.0641\n",
      "Epoch 5/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 0.0349 - mae: 0.1447 - val_loss: 0.0040 - val_mae: 0.0504\n",
      "Epoch 6/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 0.0270 - mae: 0.1253 - val_loss: 0.0057 - val_mae: 0.0607\n",
      "Epoch 7/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - loss: 0.0207 - mae: 0.1108 - val_loss: 0.0044 - val_mae: 0.0520\n",
      "Epoch 8/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 0.0169 - mae: 0.0995 - val_loss: 0.0038 - val_mae: 0.0495\n",
      "Epoch 9/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 0.0161 - mae: 0.0994 - val_loss: 0.0036 - val_mae: 0.0468\n",
      "Epoch 10/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 78ms/step - loss: 0.0137 - mae: 0.0893 - val_loss: 0.0043 - val_mae: 0.0529\n",
      "Epoch 11/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 0.0117 - mae: 0.0823 - val_loss: 0.0066 - val_mae: 0.0661\n",
      "Epoch 12/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - loss: 0.0106 - mae: 0.0803 - val_loss: 0.0031 - val_mae: 0.0433\n",
      "Epoch 13/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 0.0095 - mae: 0.0763 - val_loss: 0.0041 - val_mae: 0.0511\n",
      "Epoch 14/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - loss: 0.0092 - mae: 0.0743 - val_loss: 0.0028 - val_mae: 0.0411\n",
      "Epoch 15/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 0.0082 - mae: 0.0706 - val_loss: 0.0036 - val_mae: 0.0459\n",
      "Epoch 16/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 82ms/step - loss: 0.0080 - mae: 0.0692 - val_loss: 0.0030 - val_mae: 0.0437\n",
      "Epoch 17/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 81ms/step - loss: 0.0075 - mae: 0.0666 - val_loss: 0.0033 - val_mae: 0.0457\n",
      "Epoch 18/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 0.0077 - mae: 0.0669 - val_loss: 0.0043 - val_mae: 0.0521\n",
      "Epoch 19/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 0.0074 - mae: 0.0671 - val_loss: 0.0028 - val_mae: 0.0425\n",
      "Epoch 20/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 0.0073 - mae: 0.0663 - val_loss: 0.0047 - val_mae: 0.0548\n",
      "Epoch 21/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 80ms/step - loss: 0.0080 - mae: 0.0673 - val_loss: 0.0026 - val_mae: 0.0405\n",
      "Epoch 22/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - loss: 0.0073 - mae: 0.0655 - val_loss: 0.0029 - val_mae: 0.0417\n",
      "Epoch 23/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 90ms/step - loss: 0.0066 - mae: 0.0629 - val_loss: 0.0038 - val_mae: 0.0476\n",
      "Epoch 24/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - loss: 0.0074 - mae: 0.0672 - val_loss: 0.0034 - val_mae: 0.0441\n",
      "Epoch 25/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - loss: 0.0073 - mae: 0.0647 - val_loss: 0.0035 - val_mae: 0.0458\n",
      "Epoch 26/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - loss: 0.0074 - mae: 0.0666 - val_loss: 0.0037 - val_mae: 0.0473\n",
      "Epoch 27/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 0.0069 - mae: 0.0642 - val_loss: 0.0027 - val_mae: 0.0409\n",
      "Epoch 28/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 0.0069 - mae: 0.0644 - val_loss: 0.0032 - val_mae: 0.0440\n",
      "Epoch 29/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 82ms/step - loss: 0.0065 - mae: 0.0615 - val_loss: 0.0032 - val_mae: 0.0441\n",
      "Epoch 30/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 0.0065 - mae: 0.0623 - val_loss: 0.0026 - val_mae: 0.0403\n",
      "Epoch 31/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - loss: 0.0065 - mae: 0.0631 - val_loss: 0.0027 - val_mae: 0.0406\n",
      "Epoch 32/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - loss: 0.0063 - mae: 0.0616 - val_loss: 0.0027 - val_mae: 0.0412\n",
      "Epoch 33/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 0.0074 - mae: 0.0666 - val_loss: 0.0037 - val_mae: 0.0459\n",
      "Epoch 34/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - loss: 0.0071 - mae: 0.0650 - val_loss: 0.0030 - val_mae: 0.0421\n",
      "Epoch 35/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 0.0071 - mae: 0.0646 - val_loss: 0.0026 - val_mae: 0.0404\n",
      "Epoch 36/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 83ms/step - loss: 0.0069 - mae: 0.0640 - val_loss: 0.0031 - val_mae: 0.0424\n",
      "Epoch 37/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 0.0074 - mae: 0.0651 - val_loss: 0.0027 - val_mae: 0.0406\n",
      "Epoch 38/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - loss: 0.0073 - mae: 0.0662 - val_loss: 0.0031 - val_mae: 0.0446\n",
      "Epoch 39/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 0.0070 - mae: 0.0656 - val_loss: 0.0035 - val_mae: 0.0468\n",
      "Epoch 40/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 87ms/step - loss: 0.0059 - mae: 0.0590 - val_loss: 0.0027 - val_mae: 0.0410\n",
      "Epoch 41/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - loss: 0.0065 - mae: 0.0628 - val_loss: 0.0033 - val_mae: 0.0448\n",
      "Epoch 42/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 88ms/step - loss: 0.0058 - mae: 0.0598 - val_loss: 0.0038 - val_mae: 0.0482\n",
      "Epoch 43/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 0.0069 - mae: 0.0634 - val_loss: 0.0033 - val_mae: 0.0434\n",
      "Epoch 44/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - loss: 0.0062 - mae: 0.0595 - val_loss: 0.0026 - val_mae: 0.0401\n",
      "Epoch 45/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 78ms/step - loss: 0.0069 - mae: 0.0642 - val_loss: 0.0030 - val_mae: 0.0436\n",
      "Epoch 46/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 84ms/step - loss: 0.0059 - mae: 0.0592 - val_loss: 0.0033 - val_mae: 0.0438\n",
      "Epoch 47/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - loss: 0.0053 - mae: 0.0562 - val_loss: 0.0028 - val_mae: 0.0406\n",
      "Epoch 48/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 85ms/step - loss: 0.0059 - mae: 0.0573 - val_loss: 0.0028 - val_mae: 0.0408\n",
      "Epoch 49/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - loss: 0.0060 - mae: 0.0578 - val_loss: 0.0031 - val_mae: 0.0450\n",
      "Epoch 50/50\n",
      "\u001b[1m25/25\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 86ms/step - loss: 0.0066 - mae: 0.0640 - val_loss: 0.0026 - val_mae: 0.0410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deep model - Average test loss: 0.0049\n",
      "\n",
      "==================================================\n",
      "Model Comparison:\n",
      "==================================================\n",
      "Simple Model - Final Val Loss: 0.0024\n",
      "Deep Model - Final Val Loss: 0.0026\n",
      "\n",
      "Models saved. Check TensorBoard for detailed visualization.\n",
      "Run: tensorboard --logdir logs/scalars/\n"
     ]
    }
   ],
   "source": [
    "logdir = \"logs/scalars/\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback = keras.callbacks.TensorBoard(\n",
    "    log_dir=logdir,\n",
    "    histogram_freq=1,  \n",
    "    write_images=True,  \n",
    "    write_graph=True,   \n",
    "    update_freq='epoch', \n",
    "    profile_batch='10,20',  \n",
    "    embeddings_freq=1  \n",
    ")\n",
    "\n",
    "# Simple model\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(16, input_dim=1),\n",
    "    keras.layers.Dense(1),\n",
    "], name='simple_model')\n",
    "\n",
    "# Deep model\n",
    "model_deep = keras.models.Sequential([\n",
    "    keras.layers.Dense(64, input_dim=1, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dropout(0.2),\n",
    "    keras.layers.Dense(16, activation='relu'),\n",
    "    keras.layers.Dense(1)\n",
    "], name='deep_model')\n",
    "\n",
    "# Compile and train simple model\n",
    "model.compile(\n",
    "    loss='mse',\n",
    "    optimizer=keras.optimizers.SGD(learning_rate=0.2),\n",
    "    metrics=['mae']  # Add MAE for additional metric\n",
    ")\n",
    "\n",
    "print(\"Training simple model...\")\n",
    "training_history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=train_size,\n",
    "    verbose=1,\n",
    "    epochs=20,\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[tensorboard_callback],\n",
    ")\n",
    "print(f\"Simple model - Average test loss: {np.average(training_history.history['val_loss']):.4f}\")\n",
    "\n",
    "# Compile and train deep model with different optimizer\n",
    "logdir_deep = \"logs/scalars/deep_\" + datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tensorboard_callback_deep = keras.callbacks.TensorBoard(log_dir=logdir_deep, histogram_freq=1)\n",
    "\n",
    "model_deep.compile(\n",
    "    loss='mse',\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.01),  # Using Adam for deep model\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "print(\"\\nTraining deep model...\")\n",
    "training_history_deep = model_deep.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    batch_size=32,  # Smaller batch size for deep model\n",
    "    verbose=1,\n",
    "    epochs=50,  # More epochs for deep model\n",
    "    validation_data=(x_test, y_test),\n",
    "    callbacks=[tensorboard_callback_deep],\n",
    ")\n",
    "print(f\"Deep model - Average test loss: {np.average(training_history_deep.history['val_loss']):.4f}\")\n",
    "\n",
    "# Compare both models\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"Model Comparison:\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Simple Model - Final Val Loss: {training_history.history['val_loss'][-1]:.4f}\")\n",
    "print(f\"Deep Model - Final Val Loss: {training_history_deep.history['val_loss'][-1]:.4f}\")\n",
    "\n",
    "# Optional: Save models for later use\n",
    "model.save('simple_model.h5')\n",
    "model_deep.save('deep_model.h5')\n",
    "print(\"\\nModels saved. Check TensorBoard for detailed visualization.\")\n",
    "print(f\"Run: tensorboard --logdir logs/scalars/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fea27459-f3cd-4673-8fbf-de89ccc39df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#http://localhost:6006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d5d6e3c3-e9e0-4452-bf1b-ce64bbef62c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6007 (pid 14944), started 0:27:17 ago. (Use '!kill 14944' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-44be0f94cca4e542\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-44be0f94cca4e542\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6007;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/ --port 6007"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26237f7e-91fc-41aa-9d8a-134bc291392c",
   "metadata": {},
   "source": [
    "A brief overview of the visualizations created in this example and the dashboards (tabs in top navigation bar) where they can be found:\n",
    "\n",
    "* Scalars show how the loss and metrics change with every epoch. You can use them to also track training speed, learning rate, and other scalar values. Scalars can be found in the Time Series or Scalars dashboards.\n",
    "* Graphs help you visualize your model. In this case, the Keras graph of layers is shown which can help you ensure it is built correctly. Graphs can be found in the Graphs dashboard.\n",
    "* Histograms and Distributions show the distribution of a Tensor over time. This can be useful to visualize weights and biases and verify that they are changing in an expected way. Histograms can be found in the Time Series or Histograms dashboards. Distributions can be found in the Distributions dashboard."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ab25fc-57f9-4b65-a85e-e6c71cca1bdd",
   "metadata": {},
   "source": [
    "Breakdown of the Debugger Interface\n",
    "The Debugger Dashboard on the Tensorboard consists of five main components:\n",
    "\n",
    "* __Alerts:__ This top-left section contains a list of alert events detected by the debugger in the debug data from the instrumented TensorFlow program. Each alert indicates a certain anomaly that warrants attention. In our case, this section highlights 499 NaN/∞ events with a salient pink-red color. This confirms our suspicion that the model fails to learn because of the presence of NaNs and/or infinities in its internal tensor values.\n",
    "* __Python Execution Timeline:__ This is the upper half of the top-middle section. It presents the full history of the eager execution of ops and graphs. Each box of the timeline is marked by the initial letter of the op or graph’s name. We can navigate this timeline by using the navigation buttons and the scrollbar above the timeline.\n",
    "* __Graph Execution:__ Located at the top-right corner of the GUI, this section will be central to our debugging task. It contains a history of all the floating-type tensors computed inside graphs, i.e., the ones compiled by @tf-functions.\n",
    "* __Stack Trace:__ The bottom-right section, shows the stack trace of the creation of every single operation on the graph.\n",
    "* __Source Code:__ The bottom-left section, highlights the source code corresponding to each operation on the graph.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c32de8-23fd-4711-89c1-8aa85097a112",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlops-course)",
   "language": "python",
   "name": "mlops-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f2123a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wandb\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c6e6194",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m Failed to detect the name of this notebook. You can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33manu-mh0206\u001b[0m (\u001b[33manu-mh0206-northeastern-university\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c79021e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.3"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>D:\\MLOps_Coursework\\MLOps-course\\W&B\\wandb\\run-20251104_171629-a29lfay8</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/anu-mh0206-northeastern-university/Lab1-visualize-models/runs/a29lfay8' target=\"_blank\">xgboost-simple-custom</a></strong> to <a href='https://wandb.ai/anu-mh0206-northeastern-university/Lab1-visualize-models' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/anu-mh0206-northeastern-university/Lab1-visualize-models' target=\"_blank\">https://wandb.ai/anu-mh0206-northeastern-university/Lab1-visualize-models</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/anu-mh0206-northeastern-university/Lab1-visualize-models/runs/a29lfay8' target=\"_blank\">https://wandb.ai/anu-mh0206-northeastern-university/Lab1-visualize-models/runs/a29lfay8</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "Dataset: 366 samples, 33 features, 6 classes\n",
      "\n",
      "Training model...\n",
      "[0]\ttrain-mlogloss:1.12330\teval-mlogloss:1.20920\n",
      "[20]\ttrain-mlogloss:0.03118\teval-mlogloss:0.18994\n",
      "[40]\ttrain-mlogloss:0.02000\teval-mlogloss:0.16614\n",
      "[50]\ttrain-mlogloss:0.01899\teval-mlogloss:0.16915\n",
      "\n",
      "Train Accuracy: 1.0000\n",
      "Test Accuracy: 0.9455\n",
      "\n",
      "All visualizations logged to W&B dashboard\n",
      "Check the 'Media' tab in W&B for images\n",
      "Check the 'Charts' tab for line plots\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       1.00      1.00      1.00        34\n",
      "         1.0       0.93      0.78      0.85        18\n",
      "         2.0       1.00      0.95      0.98        22\n",
      "         3.0       0.88      0.93      0.90        15\n",
      "         4.0       0.88      1.00      0.94        15\n",
      "         5.0       0.86      1.00      0.92         6\n",
      "\n",
      "    accuracy                           0.95       110\n",
      "   macro avg       0.92      0.94      0.93       110\n",
      "weighted avg       0.95      0.95      0.94       110\n",
      "\n",
      "\n",
      "Model saved as W&B artifact\n",
      "\n",
      "View run at: https://wandb.ai/anu-mh0206-northeastern-university/Lab1-visualize-models/runs/a29lfay8\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>best_iteration</td><td>▁</td></tr><tr><td>epoch</td><td>▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>eval_loss</td><td>█▆▅▄▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>test_accuracy</td><td>▁</td></tr><tr><td>train_accuracy</td><td>▁</td></tr><tr><td>train_loss</td><td>█▆▅▄▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>best_iteration</td><td>40</td></tr><tr><td>epoch</td><td>50</td></tr><tr><td>eval_loss</td><td>0.16915</td></tr><tr><td>test_accuracy</td><td>0.94545</td></tr><tr><td>train_accuracy</td><td>1</td></tr><tr><td>train_loss</td><td>0.01899</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">xgboost-simple-custom</strong> at: <a href='https://wandb.ai/anu-mh0206-northeastern-university/Lab1-visualize-models/runs/a29lfay8' target=\"_blank\">https://wandb.ai/anu-mh0206-northeastern-university/Lab1-visualize-models/runs/a29lfay8</a><br> View project at: <a href='https://wandb.ai/anu-mh0206-northeastern-university/Lab1-visualize-models' target=\"_blank\">https://wandb.ai/anu-mh0206-northeastern-university/Lab1-visualize-models</a><br>Synced 4 W&B file(s), 5 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251104_171629-a29lfay8\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Simple but Customized XGBoost with W&B Tracking\n",
    "import wandb\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configuration - Easy to modify these parameters\n",
    "config = {\n",
    "    \"max_depth\": 6,\n",
    "    \"learning_rate\": 0.3,\n",
    "    \"n_estimators\": 100,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"seed\": 42\n",
    "}\n",
    "\n",
    "# Initialize W&B\n",
    "run = wandb.init(\n",
    "    project=\"Lab1-visualize-models\",\n",
    "    name=\"xgboost-simple-custom\",\n",
    "    config=config\n",
    ")\n",
    "\n",
    "# Load data\n",
    "print(\"Loading dataset...\")\n",
    "data = np.loadtxt('./dermatology.data', delimiter=',',\n",
    "                  converters={33: lambda x: int(x == '?'), 34: lambda x: int(x) - 1})\n",
    "\n",
    "# Prepare features and labels\n",
    "X = data[:, :33]\n",
    "y = data[:, 34]\n",
    "n_classes = len(np.unique(y))\n",
    "\n",
    "print(f\"Dataset: {len(X)} samples, {X.shape[1]} features, {n_classes} classes\")\n",
    "\n",
    "# Split data with stratification (better than simple slice)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, stratify=y, random_state=config[\"seed\"]\n",
    ")\n",
    "\n",
    "# Create XGBoost datasets\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Set up parameters\n",
    "params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': n_classes,\n",
    "    'max_depth': config[\"max_depth\"],\n",
    "    'learning_rate': config[\"learning_rate\"],\n",
    "    'subsample': config[\"subsample\"],\n",
    "    'colsample_bytree': config[\"colsample_bytree\"],\n",
    "    'seed': config[\"seed\"],\n",
    "    'eval_metric': 'mlogloss'\n",
    "}\n",
    "\n",
    "# Train with evaluation\n",
    "print(\"\\nTraining model...\")\n",
    "watchlist = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "evals_result = {}\n",
    "\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=config[\"n_estimators\"],\n",
    "    evals=watchlist,\n",
    "    evals_result=evals_result,\n",
    "    early_stopping_rounds=10,\n",
    "    verbose_eval=20\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "y_train_pred = model.predict(dtrain)\n",
    "y_test_pred = model.predict(dtest)\n",
    "\n",
    "# Calculate metrics\n",
    "train_acc = accuracy_score(y_train, y_train_pred)\n",
    "test_acc = accuracy_score(y_test, y_test_pred)\n",
    "\n",
    "print(f\"\\nTrain Accuracy: {train_acc:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Log to W&B\n",
    "wandb.log({\n",
    "    \"train_accuracy\": train_acc,\n",
    "    \"test_accuracy\": test_acc,\n",
    "    \"best_iteration\": model.best_iteration\n",
    "})\n",
    "\n",
    "# Create and log confusion matrix\n",
    "cm = confusion_matrix(y_test, y_test_pred)\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=ax)\n",
    "ax.set_title('Confusion Matrix')\n",
    "ax.set_ylabel('True Label')\n",
    "ax.set_xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Log as both image and table\n",
    "wandb.log({\n",
    "    \"confusion_matrix_plot\": wandb.Image(fig),\n",
    "    \"confusion_matrix_table\": wandb.Table(data=cm.tolist(), \n",
    "                                          columns=[f\"Pred_{i}\" for i in range(n_classes)])\n",
    "})\n",
    "plt.close(fig)\n",
    "\n",
    "# Feature importance (simple version)\n",
    "importance = model.get_score(importance_type='weight')\n",
    "if importance:\n",
    "    # Get top 10 features\n",
    "    sorted_imp = sorted(importance.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "    \n",
    "    # Plot\n",
    "    fig2, ax2 = plt.subplots(figsize=(8, 6))\n",
    "    features = [f\"f{f[0][1:]}\" for f in sorted_imp]\n",
    "    scores = [f[1] for f in sorted_imp]\n",
    "    \n",
    "    ax2.barh(features, scores)\n",
    "    ax2.set_xlabel('Importance (Weight)')\n",
    "    ax2.set_title('Top 10 Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Log both as image and as table\n",
    "    wandb.log({\n",
    "        \"feature_importance_plot\": wandb.Image(fig2),\n",
    "        \"feature_importance_table\": wandb.Table(\n",
    "            data=[[f, s] for f, s in zip(features, scores)],\n",
    "            columns=[\"Feature\", \"Importance\"]\n",
    "        )\n",
    "    })\n",
    "    plt.close(fig2)\n",
    "\n",
    "# Log learning curves with step tracking\n",
    "if evals_result:\n",
    "    fig3, ax3 = plt.subplots(figsize=(10, 5))\n",
    "    \n",
    "    # Plot training curves\n",
    "    epochs = len(evals_result['train']['mlogloss'])\n",
    "    x_axis = list(range(0, epochs))\n",
    "    \n",
    "    ax3.plot(x_axis, evals_result['train']['mlogloss'], label='Train', marker='o', markersize=3)\n",
    "    ax3.plot(x_axis, evals_result['eval']['mlogloss'], label='Test', marker='s', markersize=3)\n",
    "    ax3.set_xlabel('Boosting Round')\n",
    "    ax3.set_ylabel('Log Loss')\n",
    "    ax3.set_title('Training Progress')\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    wandb.log({\"learning_curves_plot\": wandb.Image(fig3)})\n",
    "    plt.close(fig3)\n",
    "    \n",
    "    # Also log the curves as line plots in W&B\n",
    "    for i, (train_loss, eval_loss) in enumerate(zip(evals_result['train']['mlogloss'], \n",
    "                                                     evals_result['eval']['mlogloss'])):\n",
    "        wandb.log({\n",
    "            \"train_loss\": train_loss,\n",
    "            \"eval_loss\": eval_loss,\n",
    "            \"epoch\": i\n",
    "        })\n",
    "\n",
    "print(\"\\nAll visualizations logged to W&B dashboard\")\n",
    "print(\"Check the 'Media' tab in W&B for images\")\n",
    "print(\"Check the 'Charts' tab for line plots\")\n",
    "\n",
    "# Print classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "# Save model (simple approach)\n",
    "try:\n",
    "    model_path = \"model.json\"\n",
    "    model.save_model(model_path)\n",
    "    \n",
    "    # Create W&B artifact\n",
    "    artifact = wandb.Artifact(\n",
    "        name=\"xgboost-model\",\n",
    "        type=\"model\"\n",
    "    )\n",
    "    artifact.add_file(model_path)\n",
    "    run.log_artifact(artifact)\n",
    "    print(f\"\\nModel saved as W&B artifact\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Could not save model artifact: {e}\")\n",
    "\n",
    "print(f\"\\nView run at: {run.url}\")\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (mlops-course)",
   "language": "python",
   "name": "mlops-course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
